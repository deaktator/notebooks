{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance Weighted Functions can be Deduped and Even Approximated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation Instructions\n",
    "\n",
    "`conda env create -n dev python=3.7 numpy numba scikit-learn hypothesis`\n",
    "\n",
    "`conda activate dev`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Callable, Iterable, Optional, Sequence, Tuple, TypeVar\n",
    "import numpy as np\n",
    "\n",
    "X = TypeVar(\"X\")\n",
    "Y = TypeVar(\"Y\")\n",
    "NpArray = TypeVar(\"NpArray\", bound=np.ndarray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivating Example\n",
    "\n",
    "The computation needed to compute a weighted sum can be reduced by first grouping by values and then summing the weights.  Notice the number of necessary operations is halved in this example.\n",
    "\n",
    "$$ \n",
    "\\begin{align*}\n",
    "6.3 &= (0.15 \\times 5 + 0.05 \\times 5) + (0.1 \\times 6 + 0.2 \\times 6) + (0.25 \\times 7 + 0.25 \\times 7) \\\\\n",
    "    &= (0.15 + 0.05) \\times 5 + (0.1 + 0.2) \\times 6 + (0.25 + 0.25) \\times 7 \\\\\n",
    "    &= 0.2 \\times 5 + 0.3 \\times 6 + 0.5 \\times 7\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 multiplies, 5 additions\n",
    "\n",
    "wt = np.array([0.15, 0.05,   0.10, 0.20,   0.25, 0.25])\n",
    "x  = np.array([5,    5,      6,    6,      7,    7])\n",
    "\n",
    "np.testing.assert_allclose(\n",
    "    np.dot(wt, x),\n",
    "    6.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 multiplies, 2 additions\n",
    "\n",
    "wt = np.array([0.2, 0.3, 0.5])\n",
    "x = np.array([5, 6, 7])\n",
    "\n",
    "np.testing.assert_allclose(\n",
    "    np.dot(wt, x), \n",
    "    6.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 multiplies, 2 additions\n",
    "\n",
    "np.testing.assert_allclose(\n",
    "    0.2 * 5 + 0.3 * 6 + 0.5 * 7,\n",
    "    6.3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations Under Test\n",
    "\n",
    "We want to test that an algorith is invariant to the 3 public methods, listed in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_weighted(xs: np.ndarray, dtype: np.dtype=np.float64) -> np.ndarray:\n",
    "    return np.ones(xs.shape[0], dtype=dtype)\n",
    "\n",
    "\n",
    "def remove_zero_weights(xs: NpArray, wts: np.ndarray) -> Tuple[NpArray, np.ndarray]:\n",
    "    nzi = np.nonzero(wts)[0]\n",
    "    return(xs[nzi], wts[nzi])\n",
    "    \n",
    "# TODO: To enable numba.  May or may not help.\n",
    "# NOTE: Eager compilation.\n",
    "# import numba   # requires numba conda dependency.\n",
    "# @numba.jit(\n",
    "#     [\n",
    "#         \"int32[:](int64[:], int32[:])\",\n",
    "#         \"int64[:](int64[:], int64[:])\",\n",
    "#         \"float32[:](int64[:], float32[:])\",\n",
    "#         \"float64[:](int64[:], float64[:])\"\n",
    "#     ],\n",
    "#     nopython=True, nogil=True, cache=True\n",
    "# )\n",
    "def _compressed_weights(group_ind: np.ndarray, wt: np.ndarray) -> np.ndarray:\n",
    "    # sum the weights in between indices group_ind[j] and group_ind[j+1]\n",
    "    # and associate with index j in the output.\n",
    "    \n",
    "    n = wt.shape[0]\n",
    "    m = group_ind.shape[0]\n",
    "    comp_wt = np.zeros(shape=m, dtype=wt.dtype)\n",
    "    for j in range(m - 1):\n",
    "        comp_wt[j] = np.sum(wt[group_ind[j]:group_ind[j+1]])\n",
    "\n",
    "    comp_wt[m - 1] = np.sum(wt[group_ind[-1]:n])\n",
    "\n",
    "    return comp_wt\n",
    "\n",
    "\n",
    "def _beginning_ind_contiguous_blocks(\n",
    "    xs: np.ndarray, \n",
    "    key: Optional[Callable[[NpArray], np.ndarray]]\n",
    ") -> np.ndarray:\n",
    "    # Find the indices for the beginning of contiguous runs within xs.\n",
    "    # Based on https://stackoverflow.com/a/19125898\n",
    "    if key is not None:\n",
    "        xs = key(xs)\n",
    "    return np.concatenate([[0], 1 + np.nonzero(xs[:-1] != xs[1:])[0]])\n",
    "\n",
    "\n",
    "def compressed(\n",
    "    xs: NpArray, \n",
    "    wts: Optional[np.ndarray] = None, \n",
    "    xs_commute: bool = False,\n",
    "    remove_zeros: bool = True,\n",
    "    key: Optional[Callable[[NpArray], np.ndarray]] = None\n",
    ") -> Tuple[NpArray, np.ndarray]:\n",
    "    \"\"\"Compress xs.  \n",
    "    \n",
    "    This can be weighted by wts.  If xs_commute, xs will be sorted prior\n",
    "    to compression to increase compression.\n",
    "    \n",
    "    params:\n",
    "        xs: NpArray (shape: (n,)) - Array to compress.\n",
    "        wts: Optional[np.ndarray] (default None) - if supplied, shape is (n,).\n",
    "            weights to aggregate.\n",
    "        xs_commute: bool (default False) - whether the values in xs commute\n",
    "            in the algorithm that uses them.\n",
    "        remove_zeros: bool (default True) - Whether to remove zero-weighted\n",
    "            rows prior to returning.\n",
    "        key: Optional[Callable[[NpArray], np.ndarray]] (default None) -\n",
    "            If provided, this will be used as the basis of comparison.\n",
    "    returns: Tuple[NpArray, np.ndarray]\n",
    "        compressed version of xs and wts\n",
    "        \n",
    "    usage:\n",
    "    In [1]: compressed( \n",
    "       ...:     np.rec.fromarrays( \n",
    "       ...:         [ \n",
    "       ...:             np.array([1,   0,   1,   0]),  \n",
    "       ...:             np.array([0.4, 0.3, 0.4, 0.3]) \n",
    "       ...:         ], \n",
    "       ...:         dtype=[('y', np.uint8), ('p', np.float32)] \n",
    "       ...:     ), \n",
    "       ...:     np.array([1,2,3,0]), \n",
    "       ...:     xs_commute=True \n",
    "       ...: )                                                                                                                                \n",
    "    Out[1]: \n",
    "    (rec.array([(0, 0.3), (1, 0.4)],\n",
    "               dtype=[('y', 'u1'), ('p', '<f4')]),\n",
    "     array([2, 4]))\n",
    "    In [2]: compressed( \n",
    "       ...:     np.array([0.4, 0.3, 0.4, 0.3]), \n",
    "       ...:     np.array([1,2,3,0]), \n",
    "       ...:     xs_commute=True \n",
    "       ...: )                                                                                                                                \n",
    "    Out[2]: (array([0.3, 0.4]), array([2, 4]))\n",
    "    In [3]: compressed(  \n",
    "       ...:     np.rec.fromarrays(  \n",
    "       ...:         [  \n",
    "       ...:             np.array([1,   0,   1,    0]),   \n",
    "       ...:             np.array([0.4, 0.3, 0.41, 0.3])  \n",
    "       ...:         ],  \n",
    "       ...:         dtype=[('y', np.uint8), ('p', np.float32)]  \n",
    "       ...:     ),  \n",
    "       ...:     np.array([1,2,3,0]),  \n",
    "       ...:     xs_commute=True, \n",
    "       ...:     key=lambda x: x.y \n",
    "       ...: )                                                                                                                               \n",
    "    Out[3]: \n",
    "    (rec.array([(0, 0.3), (1, 0.4)],\n",
    "               dtype=[('y', 'u1'), ('p', '<f4')]),\n",
    "     array([2, 4]))\n",
    "    \"\"\"\n",
    "    # NOTE: If xs_commute, histograms may help improve speed and\n",
    "    #       memory usage at the expense of precision.\n",
    "\n",
    "    if wts is None:\n",
    "        wts = np.ones(xs.shape[0], dtype=np.int32)\n",
    "\n",
    "    # If the xs commute, then any permutation is OK.  Sorting the\n",
    "    # array will put similar elements together, and maximize the\n",
    "    # compression when _beginning_ind_contiguous_blocks is called.\n",
    "    if xs_commute:\n",
    "        ind = xs.argsort()\n",
    "        xs = xs[ind]\n",
    "        wts = wts[ind]\n",
    "\n",
    "    group_ind = _beginning_ind_contiguous_blocks(xs, key)\n",
    "    compressed_xs = xs[group_ind]\n",
    "    if isinstance(xs, np.recarray):\n",
    "        compressed_xs = compressed_xs.view(np.recarray)\n",
    "\n",
    "    compressed_wts = _compressed_weights(group_ind, wts)\n",
    "\n",
    "    if remove_zeros:\n",
    "        compressed_xs, compressed_wts = \\\n",
    "            remove_zero_weights(compressed_xs, compressed_wts)\n",
    "    return compressed_xs, compressed_wts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Variate Creation\n",
    "\n",
    "*Very basic!*  Hypothesis does it better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_example(\n",
    "    rng: np.random.Generator, \n",
    "    xs_generator: Callable[[np.random.Generator, int], Iterable[X]],\n",
    "    min_list_size:int = 5,\n",
    "    max_list_size: int = 25,\n",
    "    min_wt_value: int = 0,\n",
    "    max_wt_value: int = 100,\n",
    "    int_weights: bool = False\n",
    "):\n",
    "    n = rng.integers(min_list_size, max_list_size + 1)\n",
    "\n",
    "    wts = (\n",
    "        rng.integers(min_wt_value, max_wt_value + 1, size=n)\n",
    "        if int_weights else\n",
    "        min_wt_value + (max_wt_value - min_wt_value) * rng.random(size=n)\n",
    "    )\n",
    "    \n",
    "    xs = xs_generator(rng, n)\n",
    "    \n",
    "    return xs, wts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties to Test\n",
    "\n",
    "These are generic tests that we can reuse across algorithms and types of equality assertions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_no_weight_like_one_weights(\n",
    "    xs: NpArray,\n",
    "    *,\n",
    "    algo: Callable[[NpArray, Optional[np.ndarray]], Y],\n",
    "    equality_assertion: Callable[[Y, Y], None]\n",
    ") -> None:\n",
    "    orig_no_weight_value = algo(xs, None)\n",
    "    one_wts = algo(xs, one_weighted(xs))\n",
    "    equality_assertion(one_wts, orig_no_weight_value)\n",
    "\n",
    "\n",
    "def assert_zero_weights_have_no_effect(\n",
    "    xs: NpArray,\n",
    "    wts: np.ndarray,\n",
    "    *,\n",
    "    algo: Callable[[NpArray, Optional[np.ndarray]], Y],\n",
    "    equality_assertion: Callable[[Y, Y], None]\n",
    ") -> None:\n",
    "    assert xs.shape[0] == wts.shape[0]\n",
    "    orig_weighted_value = algo(xs, wts)\n",
    "    pos_weights_only = algo(*remove_zero_weights(xs, wts))\n",
    "    equality_assertion(pos_weights_only, orig_weighted_value)\n",
    "    \n",
    "\n",
    "def assert_weight_consolidation_yields_same_result(\n",
    "    xs: NpArray,\n",
    "    wts: np.ndarray,\n",
    "    *,\n",
    "    algo: Callable[[NpArray, Optional[np.ndarray]], Y],\n",
    "    equality_assertion: Callable[[Y, Y], None],\n",
    "    preprocess_xs: Optional[Callable[[NpArray], NpArray]] = None,\n",
    "    xs_commute: bool = False\n",
    ") -> None:\n",
    "    assert xs.shape[0] == wts.shape[0]\n",
    "    orig_weighted_value = algo(xs, wts)\n",
    "    xs = preprocess_xs(xs) if preprocess_xs is not None else xs\n",
    "    consolidated = algo(*compressed(xs, wts, xs_commute))\n",
    "    equality_assertion(consolidated, orig_weighted_value)\n",
    "\n",
    "\n",
    "def assert_importance_weighting_properties(\n",
    "    xs: np.recarray,\n",
    "    wts: np.ndarray,\n",
    "    *,\n",
    "    algo: Callable[[np.recarray, Optional[np.ndarray]], Y],\n",
    "    equality_assertion: Callable[[Y, Y], None],\n",
    "    preprocess_xs: Optional[Callable[[NpArray], NpArray]] = None,\n",
    "    xs_commute: bool = False\n",
    ") -> None:\n",
    "    # Call the above three property tests.\n",
    "    \n",
    "    assert_no_weight_like_one_weights(\n",
    "        xs, \n",
    "        algo=algo, equality_assertion=equality_assertion\n",
    "    )\n",
    "    assert_zero_weights_have_no_effect(\n",
    "        xs, wts, \n",
    "        algo=algo, equality_assertion=equality_assertion\n",
    "    )\n",
    "    assert_weight_consolidation_yields_same_result(\n",
    "        xs, wts, \n",
    "        algo=algo, \n",
    "        equality_assertion=equality_assertion, \n",
    "        preprocess_xs=preprocess_xs,\n",
    "        xs_commute=xs_commute\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Harness\n",
    "\n",
    "*Very basic!*  Hypothesis has things like narrowing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_importance_weighting_test(\n",
    "    algo: Callable[[Iterable[X], Optional[Iterable[float]]], Y],\n",
    "    xs_generator: Callable[[np.random.Generator, int], Iterable[X]],\n",
    "    equality_assertion: Callable[[Y, Y], None],\n",
    "    num_trials: int = 1000, \n",
    "    seed: Optional[int] = None, \n",
    "    report_every: Optional[int] = None,\n",
    "    **kwargs\n",
    ") -> None:\n",
    "\n",
    "    # If seed is None, the rng will be unseeded.\n",
    "    rng = np.random.Generator(np.random.PCG64(seed))\n",
    "    for i in range(num_trials):\n",
    "        if report_every is not None and i % report_every == report_every - 1:\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "\n",
    "        xs, wts = generate_random_example(rng, xs_generator, **kwargs)\n",
    "        assert_importance_weighting_properties(xs, wts, algo=algo, equality_assertion=equality_assertion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test: Importance weighted sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function we are testing.\n",
    "def iw_sum(xs: np.ndarray, wts: Optional[np.ndarray] = None) -> float:\n",
    "    return np.sum(xs) if wts is None else np.dot(xs, wts)\n",
    "\n",
    "\n",
    "# Generator for xs that iw_sum takes.\n",
    "def iw_sum_xs_generator(min_value: int = 0, max_value: int = 100):\n",
    "    return lambda rng, n: rng.integers(min_value, max_value + 1, size=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 369 ms, sys: 14.2 ms, total: 383 ms\n",
      "Wall time: 372 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Note: no seed -> different every time.\n",
    "run_importance_weighting_test(\n",
    "    algo=iw_sum, \n",
    "    xs_generator=iw_sum_xs_generator(),\n",
    "    equality_assertion=np.testing.assert_allclose,\n",
    "    max_wt_value=1, \n",
    "    int_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test: AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc(xs: np.recarray, wts: Optional[np.ndarray] = None) -> float:\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    return roc_auc_score(xs.y, xs.p, sample_weight=wts)\n",
    "\n",
    "\n",
    "def auc_example_generator(rng: np.random.Generator, n: int) -> np.recarray:\n",
    "    # Basic version.\n",
    "    import scipy.stats\n",
    "    p = rng.random(size=n).astype(np.float32)\n",
    "    y = scipy.stats.binom.rvs(n=1, p=p, random_state=rng).astype(np.uint8)\n",
    "    return np.rec.fromarrays([y, p], dtype=[('y', np.uint8), ('p', np.float32)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 761 ms, sys: 73.6 ms, total: 835 ms\n",
      "Wall time: 906 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Note: no seed -> different every time.\n",
    "run_importance_weighting_test(\n",
    "    algo=auc, \n",
    "    xs_generator=auc_example_generator,\n",
    "    equality_assertion=np.testing.assert_allclose,\n",
    "    max_wt_value=1,\n",
    "    min_list_size=25,\n",
    "    max_list_size=50,\n",
    "    num_trials=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Property-Based Test Harness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see from the AUC code for `auc_example_generator` that the random number generation is rather well behaved.  The random variate represents a well calibrated, probabilistic classifier whose scores are uniformly distributed over \\[0, 1\\].\n",
    "\n",
    "**[Hypothesis](https://hypothesis.readthedocs.io/en/latest/)** shines at [property-based testing](https://en.wikipedia.org/wiki/Property_testing), because it is actively strives to find edge cases during variate generation.  The nice thing is that Hypothesis provides a lot of control over how expected values are sampled with power to control the edge cases, like edges of intervals, NaN, &infin;, etc.  Since the assertion functions above are parametrized by algorithm, we can write simple facades around the assertions, and create Hypothesis `given`s to inject the algorithms and data into the test suite.\n",
    "\n",
    "Note that the code below is a bit verbose, but contains some of the set up that would be beneficial to test environments like dev and CI test environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # For loading profiles.\n",
    "import hypothesis\n",
    "from hypothesis import assume, given, settings\n",
    "from hypothesis.strategies import (composite, floats, integers, just, one_of, sampled_from)\n",
    "import hypothesis.extra.numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.54.2'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Hypothesis Settings Profiles\n",
    "\n",
    "This doesn't really need to be here.  This shows how you might configure when run with pytest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See https://hypothesis.readthedocs.io/en/latest/settings.html\n",
    "# for usage on registration and loading of profiles.\n",
    "\n",
    "HYPOTHESIS_DEFAULT_PROFILE_NAME = \"ci\"  # \"default\"\n",
    "HYPOTHESIS_DEBUG_PROFILE = {\n",
    "    \"max_examples\": 10,\n",
    "    \"verbosity\": hypothesis.Verbosity.verbose\n",
    "}\n",
    "HYPOTHESIS_DEV_PROFILE = { \"max_examples\": 10 }\n",
    "HYPOTHESIS_CI_PROFILE = { \"max_examples\": 1000 }\n",
    "\n",
    "# hypothesis.Verbosity.quiet, hypothesis.Verbosity.verbose, hypothesis.Verbosity.debug\n",
    "settings.register_profile(\"debug\", **HYPOTHESIS_DEBUG_PROFILE)\n",
    "settings.register_profile(\"dev\", **HYPOTHESIS_DEV_PROFILE)\n",
    "settings.register_profile(\"ci\", **HYPOTHESIS_CI_PROFILE)\n",
    "\n",
    "# Change in CLI via:  pytest tests --hypothesis-profile <profile-name>\n",
    "# settings.load_profile(os.getenv(u\"HYPOTHESIS_PROFILE\", HYPOTHESIS_DEFAULT_PROFILE_NAME))\n",
    "settings.load_profile(\"ci\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Example Generation for AUC Example\n",
    "\n",
    "This is where hypothesis shines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "\n",
    "Data = NamedTuple('Data', [\n",
    "    ('xs', np.recarray),\n",
    "    ('wts', np.ndarray),\n",
    "])\n",
    "\n",
    "\n",
    "@composite\n",
    "def _auc_data(\n",
    "    draw, \n",
    "    add_zero_one=True,\n",
    "    min_list_size:int = 5,\n",
    "    max_list_size: int = 25,\n",
    "    min_value: float = 1e-7,\n",
    "    max_value: float = 1-1e-7,\n",
    "    min_wt_value: float = 0.00001,\n",
    "    max_wt_value: float = 10,\n",
    "    allow_nan: bool = False,\n",
    "    allow_infinity: bool = False\n",
    "):\n",
    "    y_dtype = np.uint8\n",
    "    p_dtype = np.float64\n",
    "    wts_dtype = np.float64\n",
    "\n",
    "    n = draw(integers(min_value=min_list_size, max_value=max_list_size))\n",
    "    n = n if n >= 2 or not add_zero_one else 2\n",
    "\n",
    "    p_values_strategy = floats(min_value=min_value, max_value=max_value, allow_nan=allow_nan)\n",
    "\n",
    "    p_strategy = hypothesis.extra.numpy.arrays(\n",
    "        p_dtype,\n",
    "        shape=n,\n",
    "        elements=one_of([just(0), just(1), p_values_strategy])\n",
    "    )\n",
    "    \n",
    "    y_strategy = hypothesis.extra.numpy.arrays(\n",
    "        y_dtype, \n",
    "        shape=n,\n",
    "        elements=integers(min_value=0, max_value=1)\n",
    "    )\n",
    "    \n",
    "    non_zero_wt_strategy = floats(\n",
    "        min_value=min_wt_value,\n",
    "        max_value=max_wt_value,\n",
    "        allow_nan=False,\n",
    "        allow_infinity=False,\n",
    "    )\n",
    "    \n",
    "    no_extrema_strategy = floats(\n",
    "        min_value=min_wt_value, \n",
    "        max_value=max_wt_value, \n",
    "        allow_nan=False, \n",
    "        allow_infinity=False, \n",
    "        exclude_min=True, \n",
    "        exclude_max=True,\n",
    "    )\n",
    "\n",
    "    p = draw(p_strategy) \n",
    "    y = draw(y_strategy)\n",
    "\n",
    "    wts = draw(\n",
    "        hypothesis.extra.numpy.arrays(\n",
    "            wts_dtype, \n",
    "            shape=n,\n",
    "            elements=one_of([just(0), non_zero_wt_strategy])\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # scikit-learn AUC requires one positive and one negative.\n",
    "    if add_zero_one:\n",
    "        y[0] = 0\n",
    "        y[1] = 1\n",
    "        wts[0] = draw(no_extrema_strategy)\n",
    "        wts[1] = draw(no_extrema_strategy)\n",
    "\n",
    "    xs = np.rec.fromarrays([y, p], dtype=[('y', y_dtype), ('p', p_dtype)])\n",
    "\n",
    "    return Data(xs, wts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Versions of Property Based Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypothesis Test Assumptions\n",
    "\n",
    "This shows that we can change the assumptions of the tests and that's all that really needs to change to parametrize the test with different algorithms (and assumptions in general)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "#  Uncomment this to try these assumptions, where we round the input to auc\n",
    "#  and group by the rounded version, then get the auc and compare against the\n",
    "#  original where no rounding takes place.\n",
    "#\n",
    "#  The reason we might want to do this is to round the results to get further\n",
    "#  compression.\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "def round_preprocess(n: int, debug: bool = False):\n",
    "    def preprocess(xs: np.recarray):\n",
    "        min_value = 10 ** -n\n",
    "        max_value = 1 - (10 ** -n)\n",
    "        y = xs.y\n",
    "        p = xs.p.copy()  # necessary?\n",
    "        ind = np.where((p != 0) & (p != 1))[0]\n",
    "        p[ind] = np.clip(np.round(p[ind], decimals=n), min_value, max_value)\n",
    "        a = np.rec.fromarrays([y, p], dtype=xs.dtype)\n",
    "        if debug:\n",
    "            print(f\"in preprocess({id(xs)}): {id(a)}  xs.p == a.p:  {all(xs.p == a.p)}  {a.p}\")\n",
    "        return a\n",
    "    return preprocess\n",
    "\n",
    "\n",
    "def auc_equivalent(digits: int):\n",
    "    # accurate to digits of precision.\n",
    "    return lambda x, y: np.testing.assert_allclose(x, y, rtol=10**(-digits))\n",
    "\n",
    "# This is the hard case.  Notice that round_preprocess makes the input rounded\n",
    "# prior to grouping.  It's that value that's compared against the original \n",
    "# auc of the original input.\n",
    "\n",
    "test_assumptions = {\n",
    "    \"data\":_auc_data(),  # Parametrized this\n",
    "    \"algo\": just(auc),\n",
    "    \"equality_assertion\": just(auc_equivalent(3)),\n",
    "    \"preprocess_xs\": sampled_from([round_preprocess(5, debug=False), None]),\n",
    "    \"xs_commute\": just(True)\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "#  Fill in this section to change the algorithm and parameters being tested.\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# This is the easy case when the input is not modified.\n",
    "\n",
    "# test_assumptions = {\n",
    "#     \"data\":_auc_data(),  # Parametrized this\n",
    "#     \"algo\": just(auc),\n",
    "#     \"equality_assertion\": just(np.testing.assert_allclose),\n",
    "#     \"preprocess_xs\": just(None),\n",
    "#     \"xs_commute\": just(True)\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the Hypothesis tests.  They do not need to change at all.  Just the `test_assumptions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@given(**test_assumptions)\n",
    "def test_hypothesis__no_weight_like_one_weights(data, algo, equality_assertion, *args, **kwargs):\n",
    "    assert_no_weight_like_one_weights(\n",
    "        data.xs, \n",
    "        algo=algo, equality_assertion=equality_assertion\n",
    "    )\n",
    "    \n",
    "\n",
    "@given(**test_assumptions)\n",
    "def test_hypothesis__zero_weights_have_no_effect(data, algo, equality_assertion, *args, **kwargs):\n",
    "    assert_zero_weights_have_no_effect(\n",
    "        data.xs, data.wts, \n",
    "        algo=algo, equality_assertion=equality_assertion\n",
    "    )\n",
    "\n",
    "\n",
    "@given(**test_assumptions)\n",
    "def test_hypothesis__weight_consolidation_yields_same_result(\n",
    "    data, algo, equality_assertion, preprocess_xs, xs_commute, *args, **kwargs\n",
    "):\n",
    "    assert_weight_consolidation_yields_same_result(\n",
    "        data.xs, data.wts, \n",
    "        algo=algo, \n",
    "        equality_assertion=equality_assertion, \n",
    "        preprocess_xs=preprocess_xs,\n",
    "        xs_commute=xs_commute\n",
    "    )\n",
    "\n",
    "\n",
    "# All of the above tests at once:\n",
    "\n",
    "    \n",
    "@given(**test_assumptions)\n",
    "def test_hypothesis__importance_weighting_properties(\n",
    "    data, algo, equality_assertion, preprocess_xs, xs_commute, *args, **kwargs\n",
    "):\n",
    "    assert_importance_weighting_properties(\n",
    "        data.xs, data.wts, \n",
    "        algo=algo,\n",
    "        equality_assertion=equality_assertion,\n",
    "        preprocess_xs=preprocess_xs,\n",
    "        xs_commute=xs_commute\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution of Hypothesis Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypothesis Settings Based on Global Defaults\n",
    "\n",
    "These would normally be called in a pytest test suite, but called here to show that the functions can be called outside, using the current Hypothesis settings profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 s, sys: 35.5 ms, total: 5.04 s\n",
      "Wall time: 5.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "test_hypothesis__weight_consolidation_yields_same_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.6 s, sys: 23.2 ms, total: 4.63 s\n",
      "Wall time: 4.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "test_hypothesis__zero_weights_have_no_effect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.31 s, sys: 17.8 ms, total: 4.33 s\n",
      "Wall time: 4.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "test_hypothesis__no_weight_like_one_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.42 s, sys: 19.9 ms, total: 7.44 s\n",
      "Wall time: 7.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Notice that the time is reduced when running tests simultaneously.\n",
    "test_hypothesis__importance_weighting_properties()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Hypothesis Environment Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.77 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# UNCOMMENT TO SEE DEBUG PROFILE\n",
    "# settings.get_profile(\"debug\")(test_hypothesis__no_weight_like_one_weights)()\n",
    "\n",
    "# Note this wraps test_hypothesis__no_weight_like_one_weights\n",
    "# so it can only be done once.  This probably shouldn't be done\n",
    "# in code, but through environment settings and config files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What These Tests Imply\n",
    "\n",
    "If a function, $ f $ supports importance weighting, data passed to the function can be deduplicated by summing the weights associated with the duplicated rows and rows with zero weight can removed from the input.  This means that the input can be transformed to be less than or equal to the original input size.  Information is passed from the examples, to the associated weights when grouping occurs.\n",
    "\n",
    "This further implies that if the groups are made larger, and hence the number of groups are made smaller, then the function, $ f $, through which the data passes can be sped up.  One way to achieve this is through rounding, which effectively reduces the numeric precision.  This can be thought of as passing the input to $ f $ through a step prior to the application of .\n",
    "\n",
    "Note that under this definition of support for importance weighting, we are assuming that the algorithm is commutative in its input vectors, but this may not be the case in general (imagine average run length encoding).  Note that when the inputs of the function commute, the `xs_commute` value in `compressed` can be set to true.  This will likely both speed up the compression and increase the compression ratio.\n",
    "    \n",
    "\n",
    "\n",
    "## Why Shrink Data?\n",
    "\n",
    "If $ f $ is $ \\Omega(N) $ its input vector length, compressing the input will make the algorithm's ***runtime*** based on the compressed data rather than the original full sized data.  So while the asympototic runtime of $ f $ doesn't change, the algorithm is effectively speed up through the reduction of the dimensionality of the arguments passed to $ f $.\n",
    "\n",
    "Additionally, it is often the case that the compression of data can occur in batches and be aggregated in a map/reduce style algorithm that can reduce the ***memory*** needed for the input of the function in question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
